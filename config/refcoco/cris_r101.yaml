DATA:
  dataset: refcoco
  train_lmdb: datasets/lmdb/refcoco/train.lmdb
  train_split: train
  val_lmdb: datasets/lmdb/refcoco/val.lmdb
  val_split: val
  mask_root: datasets/masks/refcoco
TRAIN:
  # Base Arch
  clip_pretrain1: pretrain/RN101.pt
  clip_pretrain2: pretrain/RN50.pt
  input_size: 480
  word_len: 20
  word_input_dim: 512
  word_dim: 1024
  vis_dim: 1024
  fpn_in: [ 512, 1024, 512 ]
  fpn_out: [ 256, 512, 1024 ]
#  sync_bn: True
  sync_bn: False

  # Decoder
  num_layers: 3
  num_head: 8
  dim_ffn: 2048
  # dropout: 0.1
  dropout: 0.1
  intermediate: False
  # Training Setting
  workers: 12  # data loader workers
  workers_val: 16
  epochs: 100
  milestones: [16]
  start_epoch: 0
  batch_size: 24  # batch size for training
  batch_size_val: 24  # batch size for validation during training, memory and speed tradeoff
  base_lr: 0.0001
  lr_decay: 0.1
  lr_multi: 0.1
  weight_decay: 0.02
  max_norm: 0
  manual_seed: 0
  print_freq: 100
  # Resume & Save
  exp_name: CRIS_R101
  output_folder: exp/refcoco
  save_freq: 1
  weight:  # path to initial weight (default: none)
#  resume:  exp/refcoco/CRIS_R101/best_model.pth # path to latest checkpoint (default: none)
#  resume:  exp/refcoco/20221128_best/best_model.pth # path to latest checkpoint (default: none)
  resume: # path to latest checkpoint (default: none)
  evaluate: True  # evaluate on validation set, extra gpu memory needed and small batch_size_val is recommend
  Distributed:
  dist_url: tcp://localhost:3681
  dist_backend: 'gloo'
  multiprocessing_distributed: False
  world_size: 1
  rank: 0
TEST:
  test_split: val-test
  test_lmdb: datasets/lmdb/refcoco/val.lmdb
#  test_split: testA
#  test_lmdb: datasets/lmdb/refcoco/testA.lmdb
#  test_split: testB
#  test_lmdb: datasets/lmdb/refcoco/testB.lmdb
#  feature_vis: True
#  visualize: True
  feature_vis: False
  visualize: False

  heatmap_save: exp/refcoco/CRIS_R101/
